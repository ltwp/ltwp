<!DOCTYPE html>
<meta charset="utf-8">
<html>
    <head>
        <title>Networks &amp; Synchronicity</title>
        <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
        <script type="text/javascript" src="http://code.jquery.com/jquery-latest.js"></script>
        <link href='http://fonts.googleapis.com/css?family=Vollkorn:400italic,700italic,400,700' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" type="text/css" href="reset.css">
        <link rel="stylesheet" type="text/css" href="css.css">

        <script type="text/javascript"
                src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        <script type="text/javascript" src="fiddles.js"></script>


    </head>
    <body>

        <h1>Networks &amp;<br /> Synchronicity</h1>
        <h3>by Lukas WinklerPrins</h3>

        <!--

TABLE OF CONTENTS
0. INTRODUCTION
What is a network?
Lay out some terms
Present guiding questions & goals

1. GRAPH AS MATRIX
Introduce adjacency matrix
define multigraph, directed graphs
Explain some notation

2. SMALL WORLD DYNAMICS
what are they trying to capture
the generation algorithm (... briefly, in full later)

3. GRAPH GENERATION ALGORITHMS
W-S
Introduce E-R
Introduce A-B
Introduce scale-free networks

4. REVIEW OF STATISTICS ON GRAPHS
how are stats generated?
eigenvalue centrality
ref: Paul Baran graphs

5. INTRO TO SYNCH
kuramoto model
how is it extended? (limited range, averaging thang)
synched app... i guess
fluxus walking piece / traffic flow activity
ref papers that discuss connectivity to synch

6. SOME INTERACTIVE SHIT RIGHT OK

-->


        <p>
            This is an web-based lesson for teaching an undergraduate, STEM-focused introduction to graph &amp; network theory, synchronicity, and random graphs.
        </p>
        <p>
            It was made as a midterm project in <a href="http://studioapma.tumblr.com">Studio APMA: Topics in Emergent Behavior</a>, a student-designed class at <a href="http://brown.edu">Brown University</a>. It uses <a href="http://www.mathjax.org">MathJax</a> for math typesetting.
        </p>
        <p>
            Thanks to Studio Applied Math, our advisor Bj&ouml;rn Sandstede, d3.js, jquery, numeric.js, and mathjax. In addition, thanks to Steven Strogatz at Cornell University, whose research frames almost all of this content.
        </p>

        <h3>How do I use this document?</h3>
        <p>
            Treat this page like a textbook. Read it through and do the exercises, marked with &#10047;. The comments, marked in the left margin by &#9758;, are clickable for some small notes by your author.
        </p>

        <p>
            Table of Contents:<br /><br />
            <a href="#hzero">0. Introduction</a><br />
            <a href="#hone">1. Graph as Matrix</a><br />
            <a href="#htwo">2. Small-World Networks</a><br />
            <a href="#hthree">3. Graph Generation</a><br />
            <a href="#hfour">4. Graph Statistics</a><br />
            <a href="#hfive">5. Synchronicity</a><br />
            <a href="#hsix">6. Final Remarks</a><br />
            <a href="#hseven">7. Interactive Force Graph</a><br />
        </p>

        <h2 id="hzero">0. Introduction</h2>

        <p>
            What is a network? Mathematicians have many names for these—some call them maps (but this term is loaded from other fields of math), some call them graphs, and some call them, well, networks<comment>"graph" is typically used in pure mathematics, and "networks" are discussed in applied subjects.</comment>. A graph consists of a series of <definition>nodes</definition> (or points, or vertices) that are connected by a series of <definition>edges</definition> (or lines). We typically visually depict a graph like this:
        </p>

        <div class="media">
            <img style="width:300px" src="images/graph.png" />
        </div>
        <div class="caption">
            Fig. 1: A graph with 10 nodes and 11 edges.
        </div>

        <p>
            We frequently will think of a graph as sets of nodes/vertices \[V={A,B,C,D,E,\ldots}\] and pairwise-defined edges \[E={(A,B),(C,D),(A,E),\ldots}\] Recall that for a set \(S\) its size<comment>"Size" of a set is also "Cardinality."</comment> or <definition>order</definition> is noted \(N=|S|\). Size is the number of nodes in the set. I will sloppily use \(|G|\) to denote the order of graph \(G\), i.e. the number of nodes.
        </p>

        <p>
            A graph is an abstract object and can represent any number of things. Nodes and edges can be cities and roads, friends and friendships, particles and their interlocking forces, sets and the functions between them. We will be discussing graphs in the general case here, but have a few framing questions relevant to many fields:
        </p>

        <p>
            What different qualitative types of graphs can we depict?<br />
            What different statistics<comment>I prefer the term "metric" to "statistic" but "metric" often has other denotations.</comment> can describe what a graph looks like?<br />
            What kinds of algorithms can make graphs with certain properties?<br /><br />
        </p>

        <p>
            And, due to the subject material for Studio Applied Math&mdash;how do graphs, or graph construction algorithms, display <em>emergent behavior</em>?<br /><br />
            Assigning a firm definition to "emergent behavior" is difficult, but broadly we can say it is the display of novel or complex behavior from a collection of simple parts<comment>I frequently refer to "parts" as "agents" or "actors."</comment>. A graph is a simple construction, but do certain graphs capture a complicated idea? Or, if we treat the node as the simple agent, is the graph the complicated collective? It is important to keep these types of questions in mind as we delve into procedures.
        </p>

        <p>
            The second part of the title, <em>Synchronicity</em>, is a layer on top of graphs. If these simple agents are connected (as a graph connects its nodes), can their behavior "synchronize" or fall into some cohesive regime? This depends on the dynamics, the Differential Equations that we assign to model each agent's behavior. How can dynamic systems interact with graphs, and what can they learn from each other? Do certain graphs lend themselves to emergent dynamics?
        </p>

        <h2 id="hone">1. Graph as Matrix </h2>

        <p>Let's imagine a five-node (\(N=5\)) graph called \(G\). \(G\) looks like this:
        </p>


        <div class="media">
            <img style="width:300px" src="images/graph_1.png" />
        </div>
        <div class="caption">
            Fig. 2: 5-node graph G.
        </div>

        <p>
            A common and efficient way of representing \(G\) in mathematical terms is as an <definition>adjacency matrix</definition>, \(A_{ij}\). In this matrix, each node represents both a row and column. All locations in the matrix are zero, unless two nodes are connected.
        </p>
        <p>
            If, for example, as in G, the <em>second</em> node is connected to the <em>fifth</em> node, then the <em>second</em> row, <em>fifth</em> column position has a value \(A_{25}=1\). If the nodes weren't connected, we would have \(A_{25}=0\).
        </p>
        <p>
            So, for our graph \(G\), our adjacency matrix is:

            $$

            A =
            \left( \begin{array}{ccccc}
            0 & 1 & 0 & 0 & 0 \\
            1 & 0 & 1 & 1 & 1 \\
            0 & 1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 & 1 \\
            0 & 1 & 0 & 1 & 0
            \end{array} \right)

            $$

        </p>
        <p>
            Up until now, we have been talking about <definition>undirected graphs</definition>, where an edge doesn't have a "direction." If they did, our matrix \(A\) would not be symmetric (across the diagonal). Node 1 pointing to node 2 would not mean that node 2 points to node 1. We are also not dealing with <definition>multigraphs</definition><comment>I have no clue where the name "multigraph" came from.</comment>, where a node can point to itself (i.e. \(A_{ii}=1\) would be possible).
        </p>
        <p>
            For the time being, we will stick with <definition>simple graphs</definition> which have no direction and no loops from a node to itself. With this framework of an adjacency matrix, a graph \(G\) is thus entirely captured by a symmetric matrix \(A_{ij}\) and all diagonal elements are \(0\). A graph with \(N\) nodes has an \(N \times N\) adjacency matrix.
        </p>
        <p>
            A graph of \(N\) nodes can have a possible \(\frac{N(N-1)}{2}\) edges.
        </p>
        <action>&#10047;&nbsp; Prove why the above line is true.</action><comment>Try induction!</comment>
        <p>
            We will use the adjacency matrix as our primary means of evaluating and manipulating graphs, as a Linear Algebraic framework provides many analysis tools.
        </p>

        <p>
            Graph theory is a rich and active field; I recommend the <a href="http://www.amazon.com/Introduction-Graph-Theory-Dover-Mathematics/dp/0486678709/ref=sr_1_1?ie=UTF8&qid=1426011034&sr=8-1&keywords=dover+graph+theory">Dover Mathematics: Introduction to Graph Theory</a> as a gentle introduction from a pure mathematics viewpoint. The study of eigenvalues and eigenvectors of an adjacency matrix \(A\) and its graph \(G\) is called Spectral Theory. Graph-theoretical frameworks prove useful in many kinds of applied analysis, including operations research, finite element methods, and network theory.
        </p>

        <h2 id="htwo">2. Small-World Networks </h2>

        <p>
            In 1998, Duncan Watts, a Ph.D student at Cornell University, and his advisor Steven Strogatz, published a <em>Nature</em> paper, <em>"Collective dynamics of ‘small-world’ networks."</em> This went on to become one of the most highly-cited Physics papers <a href="http://archive.sciencewatch.com/inter/aut/2008/08-dec/08decWattsET/">ever</a>.
        </p>
        <p>
            In it, they present a mechanism for constructing graphs ("networks") with nodes that are well-connected to nearby nodes, with a few long-distance connections tossed in. They sought to imitate networks seen in various fields of application, where graphs were not strictly structured, but not entirely random either. One simple example is a social network: you know your neighbors and people within your communities of work &amp; school, but there are also some relationships at a distance from chance encounters and other contact.
        </p>
        <p>
            Bret Victor has a beautiful and effective <a href="http://worrydream.com/#!/ScientificCommunicationAsSequentialArt">interactive page</a> for describing the Small-World effect paper.
        </p>

        <action>&#10047;&nbsp; Read through Bret Victor's page in full and play with the interactive sliders.</action>

        <p>
            The Watts-Strogatz model breaks into 3 parameters:<br /><br />
            1. \(N\), the <definition>number of vertices</definition> in a graph. Watts &amp; Strogatz use \(n\), but I prefer \(N\)<comment>Apologies if this causes any confusion.</comment>.<br />
            2. \(k\), the <definition>number of neighbors for a node</definition> to connect to, with \(k/2\) connections on each side.<br />
            3. \(p\), the <definition>probability of a neighbor edge getting "rewired."</definition><br />
        </p>

        <action>&#10047;&nbsp; Read the <a href="http://worrydream.com/refs/Watts-CollectiveDynamicsOfSmallWorldNetworks.pdf">paper</a> by Watts &amp; Strogatz and find what values of <em>n, k,</em> and <em>p</em> they were using in their computer testing.</action>
        <p>
            We will discuss the exact Watts-Strogatz (W-S) algorithm in &sect;3. The major findings of the paper boil down to this plot:
        </p>

        <div class="media">
            <img style="width:420px" src="images/L_C.png" />
        </div>
        <div class="caption">
            Fig. 3: L(p) and C(p) versus p. Image by Bret Victor.
        </div>

        <p>
            That is, path length from one node \(L\) drops off faster than the amount of clustering \(C\), as a graph becomes more random. The "small-world" effect is that just a few random connections can make the distance between nodes shrink quickly, without otherwise changing the interconnectivity very much.
        </p>
        <p>
            The \(L(p)\) and \(C(p)\) values shared in the paper correspond to premeditated \(N\) and \(k\) values&mdash;more research has been done by Watts and associates to uncover deeper relationships to \(N\) especially. A review of much of this research (and many network-theory topics) can be found in Albert &amp; Barbasi's 2002 paper <em><a href="/docs/stat_mech_complex_network.pdf">Statistical Mechanics of Complex Networks</a></em>.
        </p>


        <div class="two_col">
            <div class="leftc">
                For a network with parameters \((N, k, p)\), the total number of possible edges is \( e=\frac{N(N-1)}{2} \). <br /><br />

                \(L(p)\) is just an average shortest <em>distance</em> (in number of edges to travel) between any two nodes in our graph made with \(p\). <br /><br />

                \(C_i(p)\) is, for node \(i\), the amount of inter-connectivity among its neighbor points. It reflects how much of a fully-connected "cluster" there is around node \(i\). <br />
                with this, \(C(p)\) is just the average of all \(C_i(p)\) values.
            </div>
            <div class="rightc">
                $$

                L(p) = \frac{1}{e}\sum_{i,j} \mbox{ shortest path length from } i \mbox{ to } j

                $$


                $$

                C_i(p) = \frac{\mbox{number of neighbor edges for node } i \mbox{ not rewired}}{k(k-1)/2}

                $$


                $$

                C(p) = \frac{1}{N} \sum_i^n C_i(p)

                $$
            </div>
        </div>

        <p>
            <br />Note! The <em>expected value</em> for \(C(p)\) is just \((1-p)\). This straight line becomes the curve we see in Fig. 3 when put on a log-log scale.
        </p>

        <action>&#10047;&nbsp; Ask yourself what kinds of clusters might not lead to high <em>C(p)</em> values. Is this a good statistic for clustering?</action>

        <p>
            OK, we've seen how the Watts-Strogatz Small-World model works and the statistics they used to capture the "small-world" effect. Unfortunately the \(C(p)\) statistic is boring&mdash;I wonder if we can create a better measure for "cluster" effects<comment>In network theory these are often referred to as "cliques."</comment>? Firstly, we will consider how exactly \(C(p)\) comes out of the graph creation process.
        </p>

        <h2 id="hthree">3. Graph Generation</h2>

        <p>
            The natural question is: how did Watts and Strogatz make graphs that exhibited this statistical behavior? They carefully made an algorithm that gave them exactly what they wanted: a network that is well-connected locally with a sparse<comment>A "sparse" matrix is one where most values are zero.</comment> number of long-range connections. Note that this type of matrix has a very specific shape: a \(k/2\)-wide "band" of \(1\) values along the diagonal<comment>With some 1s at corners to complete the loop.</comment>. As \(p\) increases, this band dissolves.
        </p>

        <div class="media">
            <img style="width:150px" src="images/regular.png" />
        </div>
        <div class="caption">
            Fig. 4: A regular neighbor network with 20 nodes and <em>k=4</em>. Image from the Watts-Strogatz paper.
        </div>

        <p>
            The above graph has an adjacency matrix as such:
            $$
            \left( \begin{array}{cccccccccccccccccccc}
            0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
            1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
            1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 \\
            1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 \\
            1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0
                \end{array} \right)
            $$

            See it? See the big diagonals of ones, around the center diagonal of zeros? It can be difficult to visually see with text, so sparse matrices with values of only \(0\) or \(1\) are often visualized as <em>spy diagrams</em>, where a dot is placed in an x-y plane to denote a value 1. As a spy graph, this matrix looks like this:
        </p>

        <div class="media">
            <img style="width:500px" src="images/spy.png" />
        </div>
        <div class="caption">
            Fig. 5: Spy Diagram of regular neighbor network.
        </div>

        <p>
            It is very easy to create this lattice-like graph. However, as \(p\) increases, the adjacency matrix' strict structure degrades. Let us walk through the W-S mechanism for creating small-world networks.
        </p>

        <h3>The Watts-Strogatz Algorithm</h3>

        <p>
            Let us do an example with \((N, k, p)=(7,4,0.1)\).
        </p>

        <div class="two_col">
            <div class="leftc">
                At first, our 7-node, 4-neighbor adjacency matrix clearly reflects the neighbor connections.<br /><br />

                After this, we randomly rewire the closest clockwise neighbor for each node (and then second-closest, and third-closest, etc. up to \(k/2\)). When we rewire, we remove the existing edge and connect the node in question to a node it does not already share an edge with.
            </div>
            <div class="rightc">
                $$
                \left( \begin{array}{ccccccc}
                0 & 1 & 1 & 0 & 0 & 1 & 1 \\
                1 & 0 & 1 & 1 & 0 & 0 & 1 \\
                1 & 1 & 0 & 1 & 1 & 0 & 0 \\
                0 & 1 & 1 & 0 & 1 & 1 & 0 \\
                0 & 0 & 1 & 1 & 0 & 1 & 1 \\
                1 & 0 & 0 & 1 & 1 & 0 & 1 \\
                1 & 1 & 0 & 0 & 1 & 1 & 0
                \end{array} \right)
                $$
            </div>
        </div>
        <br />
        <div class="two_col">
            <div class="leftc">
                We have a \(p=0.1\) so only about a tenth of our connections will get rewired. Let's say that \(A_{3,4}=0\) and is replaced by \(A_{3,6}=1\). <br /><br />

                On our second go-around (rewiring second-nearest neighbors), we remove \(A_{6,1}\) and replace it with \(A_{6,2}\). These are our only two rewirings, and our updated adjacency matrix is complete.
            </div>
            <div class="rightc">
                $$
                \left( \begin{array}{ccccccc}
                0 & 1 & 1 & 0 & 0 & 0 & 1 \\
                1 & 0 & 1 & 1 & 0 & 1 & 1 \\
                1 & 1 & 0 & 0 & 1 & 1 & 0 \\
                0 & 1 & 0 & 0 & 1 & 1 & 0 \\
                0 & 0 & 1 & 1 & 0 & 1 & 1 \\
                0 & 1 & 1 & 1 & 1 & 0 & 1 \\
                1 & 1 & 0 & 0 & 1 & 1 & 0
                \end{array} \right)
                $$
            </div>
        </div>

        <h3>The Albert-Barb&aacute;si Model</h3>

        <p>
            Also in the late 90s, R&eacute;ka Albert (now at Penn State) and Albert L&aacute;szl&oacute;-Barb&aacute;si (now at Northeastern) were studying graph properties of many naturally occuring networks in social and biological realms and used one crucial statistical measure to frame their approach, the <definition>degree distribution</definition> \( P(k) \).
        </p>
        <p>
            \( P(k) \) represents the proportion of all nodes in a graph with <definition>degree</definition> \(k\), where degree is the number of connected edges. Thus \(P(k=2)\) is the proportion of nodes with degree two. We let \(k_i\) represent the degree of node \(i\).
        </p>
        <p>
            From an adjacency matrix \(A_{ij}\), we can define
            $$
            k_i = \sum_{j}^N A_{ij}
            $$
        <p>
            Albert and Barb&aacute;si found that most of these naturally-occuring networks had degree distributions that were approximate power laws, that is
            $$

            P(k) ~ k^{-\gamma}

            $$
            Where \(\gamma > 0\) generally, and \(\gamma\) depends on the exact network but has typically been between 2 and 3. A network with power-law degree distribution is said to be <definition>scale-free</definition><comment>I don't quite know where this name came from, but I assume it is because a power relationship has no inherent length scale.</comment>.
        </p>
        <p>
            The authors created their own algorithm for generating graphs which produce this kind of degree distribution, which we will now detail.
        </p>
        <p>
            The model begins with \(m_0\) initial nodes, and they are a complete graph (i.e. every node is connected to every other node).
        </p>
        <p>
            Next, nodes are incrementally added. For each added node \(i\), the probability that \(i\) will be connected to node \(j\) is
            $$

            \Pi = \frac{k_j}{\sum_{m}^{n} k_m}

            $$
        </p>
        <p>
            Nodes are added until the final desired total \(N\) is achieved. This definition for \(\Pi\) is called <definition>preferential treatment</definition>&mdash;nodes with more connections are also likely to gain additional ones when new nodes are added. This phenomena can be seen in many natural systems (e.g. it is easier to make new friends if you already have many).
        </p>

        <p>
            We will now walk through the addition of two nodes starting from a graph with \(m_0 = 4\), the only defining parameter. (Remember, the Watts-Strogatz model has two, \(k\) and \(p\).)
        </p>

        <div class="two_col">
            <div class="leftc">
                We begin with \(N=m_0=4\) nodes, fully connected.
            </div>
            <div class="rightc">
                $$
                \left( \begin{array}{cccc}
                0 & 1 & 1 & 1 \\
                1 & 0 & 1 & 1 \\
                1 & 1 & 0 & 1 \\
                1 & 1 & 1 & 0 \\
                \end{array} \right)
                $$
            </div>
        </div>
        <div class="two_col">
            <div class="leftc">
                Now, we add a new fifth node. We calculate its probabilities of connection simultaneously, so it has \(\frac{3}{12} = \frac{1}{4} = 0.25\) chance of being connected to any of the four existing nodes. Let us suppose only a connection to the third node is made.
            </div>
            <div class="rightc">
                $$
                \left( \begin{array}{ccccc}
                0 & 1 & 1 & 1 & 0 \\
                1 & 0 & 1 & 1 & 0 \\
                1 & 1 & 0 & 1 & 1 \\
                1 & 1 & 1 & 0 & 0 \\
                0 & 0 & 1 & 0 & 0
                \end{array} \right)
                $$
            </div>
        </div>
        <div class="two_col">
            <div class="leftc">
                We will now add a sixth and final node. Its probabilities of connection to each of the previous 5 nodes, respectively, are<br />
                \(\frac{3}{14}\), \(\frac{3}{14}\), \(\frac{4}{14}\), \(\frac{3}{14}\), and \(\frac{1}{14}\). Let connections to the 3rd and 4th nodes be made. <br /><br />

                And so concludes our graph construction via the A-B model.
            </div>
            <div class="rightc">
                $$
                \left( \begin{array}{cccccc}
                0 & 1 & 1 & 1 & 0 & 0 \\
                1 & 0 & 1 & 1 & 0 & 0 \\
                1 & 1 & 0 & 1 & 1 & 1 \\
                1 & 1 & 1 & 0 & 0 & 1 \\
                0 & 0 & 1 & 0 & 0 & 0 \\
                0 & 0 & 1 & 1 & 0 & 0
                \end{array} \right)
                $$
            </div>
        </div>

        <div class="media">
            <img style="width:300px" src="images/a-b.png" />
        </div>
        <div class="caption">
            Fig. 6: A graph with <em>N=8</em> and <em>m_0 = 3</em> from the A-B process.
        </div>
        <!-- *** make a A-B graph image! -->

        <h3>The Erd&#337;s-R&eacute;yni Model</h3>

        <p>
            Yes, another two-name graph model<comment>These are sometimes also called "Bernoulli Graphs," but I don't know why.</comment>! This one is particularly easy, however, and set the stage early for further research in statistics on graphs. In the mid-20th century, Erd&#337;s and R&eacute;yni, put forth the foundations of random graph theory. A brief history and research review is also given in the aforementioned <a href="docs/stat_mech_complex_network.pdf">Statistical Mechanics on Complex Networks</a> PDF.
        </p>
        <p>
            In their formulations, a <definition>Random Graph</definition> of size \(N\) connects two nodes with fixed probability \(p\). These graphs end up looking very stringy for low \(p\) values, with many unconnected nodes or disconnected groups. Much of the study by Erd&#337;s and R&eacute;yni focused on how properties of the graph change as \( N \rightarrow \infty \).
        </p>
        <p>
            They found that many properties&mdash;like size of the largest component, for instance&mdash;change their value suddenly, at specific relationships between \(N\) and \(p\). It is hence we might call these properties <em>emergent</em>, arising suddenly from otherwise innocuous rules.
        </p>

        <div class="media">

            <img style="width:900px" src="images/compare.jpg" />

        </div>
        <div class="caption">
            Fig. 7: Comparison of W-S <em>(N=8,p=0.3,k=2)</em>, A-B <em>(N=8,m_0=3)</em>, and E-R <em>(N=8,p=0.3)</em> graph models.
        </div>

        <p>
            I have now laid out algorithms for the W-S, A-B, and E-R graph generation models. I will refer to them by these acronyms henceforth.
        </p>

        <action>&#10047;&nbsp; Consider your own mechanism for graph generation and simulate it by hand or programmatically.</action>

        <h2 id="hfour">4. Graph Statistics</h2>

        <p>
            We saw in &sect;2 that Watts &amp; Strogatz created their algorithm as they did towards some statistical goals&mash;the average bath length \(L\) and clustering coefficient \(C\), both given \(p\) dependencies. By tuning or creating from scratch our graph-construction algorithms, we can achieve a great variety of qualitatively different graphs. Statistics are often excellent ways to quantify these differences.
        </p>

        <action>&#10047;&nbsp; Read from the bottom of page 4 onwards <a href="docs/newman.pdf">The Mathematics of Networks</a> by Newman, focusing on notions of centrality.</action>

        <p>
            Let us review the three key statistics we have examined for graphs, on a graph \(G\) with adjacency matrix \(A_{ij}\).
            $$
            P(k) = \frac{\mbox{ number of nodes with degree k }}{|G|}
            $$
            $$
            C_i(p) = \frac{\mbox{number of neighbor edges for node } i \mbox{ not rewired}}{k(k-1)/2}
            $$
            $$
            C(p) = \frac{1}{n} \sum_i^n C_i(p)
            $$
            $$
            L(p) = \frac{1}{e}\sum_{i,j} \mbox{ shortest path length from } i \mbox{ to } j
            $$

        </p>
        <p>
            Outside of the W-S rewiring mechanism for network creation, our notion of clustering does not make sense. What if \(C\) represented the proportion of nodes in a strongly-connected component<comment>Strongly-Connected Components are als ocalled "cliques," most often in computer science applications.</comment>? (A strongly-connected component is a grouping of nodes in which all nodes are connected to all others.)
        </p>

        <div class="media">
            <img style="width:300px" src="images/clique.png" />
            <!-- clique -->
        </div>
        <div class="caption">
            Fig. 8: Highlighted is a strongly-connected component in a graph <em>G</em>.
        </div>

        <p>
            Of importance to Erd&#337;s and R&eacute;yni was the <definition>diameter</definition> of a graph: the longest shortest path between any two points. If a graph is disconnected, this should be \(\infty\) but is normally taken to be the largest diameter of the components. And this was just one statistic of many! Different statistics become relevant depending on the questions we are asking and problems we are trying to solve.
        </p>

        <action>&#10047;&nbsp; OPTIONAL: Create your own "clustering" statistic. It can vary by node or be a bulk statistic. Test it on some graphs.</action>

        <action>&#10047;&nbsp; Adjust your previous algorithm or create a new algorithm that makes your clustering coefficient increase, linearly or otherwise, with \(N\), the size of your graph. Test using software or by hand.</action>

        <h2 id="hfive">5. Synchronicity</h2>

        <div class="media">
            <iframe width="420" height="315" src="https://www.youtube.com/embed/5v5eBf2KwF8" frameborder="0" allowfullscreen></iframe>
        </div>
        <div class="caption">
            Fig. 9: Metronomes synchronizing on a piece of suspended foam.
        </div>

        <p>
            What is happening in this video? The metronomes, simple dynamic pendulums, fall into a state of synchronization with each other, seemingly regardless of their random starting arrangements. How are they communicating&mdash;or what is the structure of their inter-connectedness?
        </p>

        <p>
            We now enter into discussion of another dimension to explore&mdash;what happens when we lay a set of differential equations over a graph? Let each node represent a state variable \[x_i, i\in{1,2,3,...}\] and its set of edges reflect the form of the state variable's governing equation. Here, we enter into rich territory of exploring the relationships between dynamics of the system and structural properties of the graph.
        </p>
        <p>
            One example, regularly returned to due to popularization in Steven Strogatz' book on <a href="http://www.amazon.com/Nonlinear-Dynamics-Chaos-Applications-Nonlinearity/dp/0738204536/ref=sr_1_1?ie=UTF8&qid=1426285105&sr=8-1&keywords=strogatz+nonlinear">Nonlinear Dynamics</a>, is the <definition>Kuramoto Oscillator</definition>.
        </p>
        <p>
            Let us have \(N\) state variables, \(\theta_i\) where \(\theta_i \in [0,2\pi)\) is periodic. Our dynamics are governed by:

            $$
            \dot{\theta}_i = \omega_i + \sum_{j=1}^N \Gamma_{ij}(\theta_j - \theta_i)
            $$

            where \(\omega\) represents the natural frequency of oscillator \(i\), and \(\Gamma_{ij}(x)\) couples one oscillator's dynamics to those of all others.
        </p>
        <p>
            In the Kuramoto model,
            $$
            \Gamma_{ij}(\theta_j - \theta_i) = \frac{K}{N}\sin{\theta_j - \theta_i}
            $$

            where \(K\) is a constant, the <definition>coupling coefficient</definition> for the system.
        </p>
        <p>
            <em>OK&mdash;what is going on here?</em> The speed at which the <em>i</em>th oscillator moves (\(\dot{\theta}_i\) is described. This value starts from a fixed given \(\omega_i\) and is further pulled around based on how close it is, on the unit circle \([0,2\pi)\), to all other oscillators. An oscillator will try to "catch up" or "slow down" to meet other oscillators (sending \(\theta_j-\theta_i \rightarrow 0\)), and the rate at which it adjusts to its peers is dictated by the value \(K\).
        </p>
        <p>
            \(\omega_i\) values are generally assumed to come from a unimodal symmetric distribution \(g(\omega)\).<comment>Interesting changes in system dynamics occur when other distributions are used, however.</comment>
        </p>
        <p>
            Kuramoto's key insight in its analysis was to see that each oscillator's dependence on all other oscillators could be simplified. Let us define, in the complex plane, two values \(r\) and \(\psi\) by the equation
            $$
            re^{\sqrt{-1}\psi} = \frac{1}{N}\sum_j^N e^{\sqrt{-1}\theta_j}
            $$

            where I am using \(\sqrt{-1}\) as the imaginary number instead of \(i\) to reserve \(i\) for index notation.
        </p>
        <p>
            Now let us multiply both sides by \(e^{-\sqrt{-1}\theta_i}\):
            $$
            re^{\sqrt{-1}(\psi-\theta_i)} = \frac{1}{N}\sum_j^N e^{\sqrt{-1}(\theta_j-\theta_i)}
            $$

            Now, if we focus only on the imaginary part, we find:
            $$
            r\sin{(\psi-\theta_i)} = \frac{1}{N}\sum_j^N \sin{(\theta_j - \theta_i)}
            $$
        </p>
        <p>
            With this, we can see that for an individual oscillator,

            $$
            \dot{\theta}_i = \omega_i + Kr\sin{(\psi - \theta_i)}
            $$
        </p>
        <p>
            Thus we have found that each oscillator is really just interacting with the <em>bulk</em> properties of the \(N\) oscillators&mdash;only the averages, \(r\) and \(\psi\) affect an oscillator's dynamics<comment>Remember, are values are time-dependent so both sides of the equation should be evaluated at the same <em>t</em> value!</comment>.
        </p>
        <p>
            Per Sebastian Skardal has made an excellent stand-alone Kuramoto model visualization app called <a href="https://sites.google.com/site/persebastianskardal/software/synched">Synched</a>.
        </p>

        <action>&#10047;&nbsp; Review how the Kuramoto system works and how it breaks down into mean properties. Download &amp; try "Synched."</action>

        <p>
            Many questions follow. For a given \(g(\omega)\) distribution, does there exist a stable-state solution, where \(\dot{\theta}_i = 0 \forall i\)? If so, is it stable or unstable? These questions are yet unanswered in completely general cases, but some specific extensions of the Kuramoto model yield fruit.
        </p>
        <p>
            <a href="docs/wiley.pdf">Wiley, Strogatz, and Girvan</a> allow \(K=N\) and for the sum to be specified only in a range \(k\) wide on either side of the node of interest \(i\). That is,
            $$
            \dot{\theta}_i = \omega_i + \sum_{j=i-k}^{i+k} \sin(\theta_j - \theta_i)
            $$
        </p>
        <p>
            Here our earlier work of graphical methods becomes relevant. Instead of the oscillator being coupled to <em>all others</em>, just connect it to its \(2k\) nearest neighbors. This is akin to a highly-ordered (\(p=0\)) W-S graph.
        </p>

        <div class="media">
            <img style="width:300px" src="images/neighbors.png" />
        </div>
        <div class="caption">
            Fig. 10: Around a set of <em>N</em> nodes, node <em>i</em> is only connected to its nearest neighbors.
        </div>

        <p>
            Now we have another layer of framework to ask the above questions. Will the existence of a stable equilibrium be affected by rewirings of the graph, with \(p\noteq 0\)? How stable is the equilibria, and are there bifurcation points in our parameter space? These questions and more are explored further in the aforementioned Wiley and co. paper.
        </p>
        <p>
            Most results of research in this field are complicated either in analysis or computation and are outside of the scope of this page. As a starting point, see another Strogatz paper, <a href="docs/sync_strogatsz.pdf">From Kuramoto to Crawford</a>. Recall: this is only <em>one dynamic system</em>! The possibilities for others are literally endless.
        </p>

        <h2 id="hsix">6. Final Remarks</h2>

        <p>
            This lesson is meant to provoke curiosity and questions in a few directions. What mechanisms do we have for creating and describing networks? How intertwined are our generation algorithms and the processes by which we evaluate them? What can structural properties of networks tell us about dynamics that may occur on them?
        </p>
        <p>
            Paul Baran, one of the forefathers of the modern internet, put forth a theory and justification for a distributed network of computers in his 1964 paper, <a href="http://www.rand.org/content/dam/rand/pubs/research_memoranda/2006/RM3420.pdf">On Distributed Communications</a>. At one point, the following image appears:

        <div class="media">
            <img style="width:800px" src="images/network_types.png" />
        </div>
        <div class="caption">
            Fig. 11: Baran's three types of networks.
        </div>

        <p>
            We all intuitively, instantly, recognize the difference between these. The first has one central node, the next is formed by a few small packages, and the last a fully spread network. But what if a computer had to differentiate them? This is my last question for you.
        </p>

        <action>&#10047;&nbsp; Try to create a statistic that clearly delineates the three types of networks Baran separates.</action>

        <h2 id="hseven">7. Interactive Force Graph</h2>

        <p>
            Using <a href="http://d3js.org">D3</a>, I made an interactive sandpit to create graphs. Click and drag from nodes to create new nodes or new edges, use backspace/delete to remove your selections, and watch statistics change.
        </p>
        <p>
            Go to the interactive graph <a href="graph.html">HERE</a>.
        </p>
        <p>
            That's all! Thanks for reading. Send any thoughts, comments, or criticism to <em>lukas@brown.edu</em>.
        </p>

    </body>
    <!--
TO DO LIST:
-----------

/ Image for a "graph"
/ Image for labeled graph
/ define degree of a vertex in section 1
/ make *action items/questions* for each section
X image/graphic for lengths & clustering
/ adjacency walk-through stuff for A-B model
X make terms link to their definitions ?
+ oscillator visualization
/ work in references to PDFs
/ kuramoto near neighbors images
/ A-B graph drawing / spy drawing
/ image of random graph
/ comparison of W-S, A-B, E-R
/ clique image

+ linear algebra in JS
+ metrics generation in JS
+ metrics display in JS

--->
</html>
